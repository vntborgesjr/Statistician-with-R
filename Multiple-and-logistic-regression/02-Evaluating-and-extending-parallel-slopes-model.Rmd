---
title: "02 - Evaluating and extending parallel slopes model"
output: 
  html_notebook:
    toc: true
    toc_float: false
    toc_depth: 4
    code_folding: hide
---

```{r}
library(tidyverse)
mpg <- ggplot2::mpg
mpg_manual <- mpg %>% 
  filter(trans %in% c('manual(m5)', 'manual(m6)'))
library(openintro)
data(mariokart)
mario_kart <- mariokart %>% 
  filter(total_pr < 100)
rm(mariokart)
colnames(mario_kart) <- c("ID", "duration", "nBids", "cond", "startPr", "shipPr", "totalPr", "shipSp", "sellerRate", "stockPhoto", "wheels", "title")
SAT <- read.csv('/home/cla/Documentos/Vitor/DataCamp/Statistician-with-R//Datasets/SAT.csv')
```

## **Model fit, residuals, and prediction**

**1. Model fit, residuals, and prediction**

**2. Residuals**

One of the most natural questions we might have about our model is: "how well does it fit?" We measure this by computing---for each observation---the difference between the actual value of the response variable and the fitted value from our model. This distance is called a residual. Residuals for two observations are illustated in this plot with vertical arrows. Just as in simple linear regression, the model fitting procedure will automatically minimize the length of those arrows across all of the points. In a parallel slopes model, our model is represented by two lines, but only one of the lines is relevant for each observation. In the plot, the green points---each corresponding to a car made in 2008---are compared to the green line, and the orange points---each corresponding to a car made in 1999---are compared to the orange line. Note that this necessarily makes our parallel slopes model more flexible than a simple linear regression model. The two lines are constrained to be parallel, but we still have two instead of one. If the best fit occurs when the two lines are very close together, then the coefficient of the categorical variable will be very small, and the parallel slopes model will be very similar to the simple linear regression model.

```{r, echo = FALSE}
ggplot(data = augment(lm(hwy ~ displ + factor(year), data = mpg)), aes(x = displ, y = hwy, color = `factor(year)`)) +
  geom_point() +
  geom_line(aes(y = .fitted)) +
  geom_segment(aes(x = c(6.5), xend = c(6.5), y = c(12), yend = c(17.5)), arrow = arrow(ends = 'first', type = 'closed', length = unit(0.1, 'cm')), color = "red") +
  geom_segment(aes(x = c(6.2), xend = c(6.2), y = c(14.8), yend = c(25)), arrow = arrow(ends = 'first', type = 'closed', length = unit(0.1, 'cm')), color = "blue") 
```

**3. Model Fit**

The coefficient of determination---usually called R squared---carries over from simple linear regression. Recall that the sum of the squared residuals (or errors) is denoted SSE. If the model fits the data better, then the residuals are smaller, the SSE is smaller, and the R squared value is higher. The total sum of the squares---denoted SST---is a function of the response variable alone and does not depend on the model. In general, a higher R squared may be a sign of a better model fit, but the situation becomes more complicated in multiple regression because additional explanatory variables will always increase R squared. Thus, model fits in multiple regression are often compared using the adjusted R squared value defined here. Note that the only difference is that a penalty is applied as the number of explanatory variables p increases. Unlike R squared, adjusted R squared will not necessarily increase as new explanatory variables are added---it could go up or down.

**4. Fitted values**

Retrieving the fitted values produced by our model can be done in two different ways. The predict() function will return the fitted values as a vector, while the augment() function from the broom package will return a data.frame that contains the original observations, the fitted values, the residuals, and several other diagnostic computations. Since this course uses the tidyverse, we will generally prefer the latter method. Another important bit of functionality is the ability to make out-of-sample predictions. This means using our model to make predictions about observations that were not part of the data set on which the model was fit.

**5. 2008 Toyota Matrix**

For example, I currently drive a 2008 Toyota Matrix like the one you see here. However, this car isn't in our dataset. What does our model predict for the fuel economy of this car?

**6. Predictions**

To figure this out, we can simply input the values of the explanatory variables. In this case, my car has a 1 point 8 liter engine. We can create a new data.frame with the information, and then feed it to predict() or augment() using the newdata argument. Either function will return a predicted mileage of 30 point 18 mpg. This is slightly less than the manufacturer's reported 33 mpg, giving us a residual of 2 point 82 mpg.

```{r}
new_obs = data.frame(displ = 1.8, year = 2008)
# returns a vector
augment(lm(hwy ~ displ + factor(year), data = mpg), newdata = new_obs, se_fit = TRUE)
```

**7. Let's practice!**

You'll get the opportunity to explore this on your own in the next set of exercises.

### **R-squared vs. adjusted R-squared**

Two common measures of how well a model fits to data are 
$R^2$ (the coefficient of determination) and the adjusted 
$R^2$. The former measures the percentage of the variability in the response variable that is explained by the model. To compute this, we define
 $$R^2 = 1 - \frac{SSE}{SST}$$
where $SSE$ and $SST$ are the sum of the squared residuals, and the total sum of the squares, respectively. One issue with this measure is that the $SSE$ can only decrease as new variable are added to the model, while the $SST$ depends only on the response variable and therefore is not affected by changes to the model. This means that you can increase $R^2$ by adding any additional variable to your modelâ€”even random noise.

The adjusted $R^2$ includes a term that penalizes a model for each additional explanatory variable (where *p*is the number of explanatory variables).
 $$R^2_{adj} = 1 - \frac{SSE}{SST}.\frac{n - 1}{n - p - 1}$$,
 
We can see both measures in the output of the `summary()` function on our model object.

**Instructions**

- Use `summary()` to compute $R^2$ and adjusted $R^2$ on the model object called `mod`.
- Use `mutate()` and `rnorm()` to add a new variable called `noise` to the `mario_kart` data set that consists of random noise. Save the new dataframe as `mario_kart_noisy`.
- Use `lm()` to fit a model that includes `wheels`, `cond`, and the random noise term.
- Use `summary()` to compute $R^2$ and adjusted $R^2$ on the new model object. Did the value of $R^2$ increase? What about adjusted $R^2$?

```{r}
# R^2 and adjusted R^2
summary(mod)

# add random noise
mario_kart_noisy <- mario_kart %>% 
  mutate(noise = rnorm(n = nrow(mario_kart)))
  
# compute new model
mod2 <- lm(totalPr ~ wheels + cond + noise, data = mario_kart_noisy)

# new R^2 and adjusted R^2
summary(mod2)
```

### **Prediction**

Once we have fit a regression model, we can use it to make predictions for unseen observations or retrieve the fitted values. Here, we explore two methods for doing the latter.

A traditional way to return the fitted values (i.e. the $\hat{y}$'s) is to run the `predict()` function on the model object. This will return a vector of the fitted values. Note that `predict()` will take an optional `newdata` argument that will allow you to make predictions for observations that are not in the original data.

A newer alternative is the `augment()` function from the `broom` package, which returns a `data.frame` with the response varible ($y$), the relevant explanatory variables (the $x$'s), the fitted value ($\hat{y}$) and some information about the residuals ($e$). `augment()` will also take a `newdata` argument that allows you to make predictions.

**Instructions**

The fitted model `mod` is already in your environment.

- Compute the fitted values of the model as a vector using `predict()`.
- Compute the fitted values of the model as one column in a `data.frame` using `augment()`.

```{r}
# return a vector
predict(mod)

# return a data frame
augment(mod)
```

## **Understanding interaction**

**1. Understanding interaction**

**2. Interaction**

Thus far we have considered models where the regression lines were constrained to be parallel. But what if the lines didn't have to be parallel? In this plot, we illustrate the model when the lines are allowed to have their own slopes. Now, the relationship between fuel economy and engine size is not the same for the newers cars as it is for the older cars. The slope of that relationship now changes based on the year. Thus, engine size and year are allowed to *interact* in their relationship with fuel economy.

```{r}
ggplot(data = mpg, aes(x = displ, y = hwy, color = factor(year))) +
  geom_point() +
  geom_smooth(se = FALSE, method = 'lm')
```

**3. Adding interaction terms**

Mathematically, we achieve this by adding yet another term to our model. This third explanatory variable is the product of displ and is_newer, and it is called an interaction term. The addition of this term results in the two regression lines shown above. Note that unlike in the previous model, now both the intercepts and the slopes are different.

**4. Interaction syntax**

In R, we can add an interaction term using some new syntax. Here, we simply add a third term to our model, using the colon to denote multiplication of the explanatory variables in the interaction term. This produces a model with four coefficients: one for the intercept, one for displacement, one for the year, and one for the interaction of displacement with year.

```{r, echo = FALSE}
lm(hwy ~ displ + factor(year) + displ:factor(year), data = mpg)
```

**5. Reasoning about interaction**

Interpreting the coefficients in an interaction model becomes more complicated. Note the difference between the fitted coefficients of the parallel slopes model with the interaction model. The original slope of -3 point 61 mpg per liter for all cars is now separated into two slopes: -3 point 77 mpg per liter for older cars, and -3 point 46 mpg per liter for newer cars. Thus, fuel economy for the older cars is not only lower overall, but it also declines more rapidly as a function of engine size. This importantly changes the interpretation of the model. It suggests that the greater fuel economy of the 2008 cars is not just related to the fact that they had smaller engines, on average---a function mainly of consumer choice. It suggests that the 2008 cars were also engineered better, in that they were able to maintain fuel economy slightly better even with larger engine sizes. Now, the size of that difference is small, and as the plot suggests, it doesn't appear to make a practical difference. In this case, the difference is not statistically significant, and thus the parallel slopes model is probably a better representation of the data, but you will tackle that subtlety in another course.

```{r}
lm(hwy ~ displ + factor(year), data = mpg)

lm(hwy ~ displ + factor(year) + displ:factor(year), data = mpg)
```

**6. Let's practice!**

Let's see some more examples in the next exercises.

### **Fitting a model with interaction**

Including an interaction term in a model is easyâ€”we just have to tell `lm()` that we want to include that new variable. An expression of the form

    `lm(y ~ x + z + x:z, data = mydata)`
    
will do the trick. The use of the colon (`:`) here means that the interaction between $x$ and $z$ will be a third term in the model.

**Instructions**

The data frame `mario_kart` is already loaded in your workspace.

- Use `lm()` to fit a model for the price of a MarioKart as a function of its condition and the duration of the auction, *with interaction*.

```{r}
# include interaction
lm(totalPr ~ cond + duration + cond:duration, data = mario_kart)
```

### **Visualizing interaction models**

Interaction allows the slope of the regression line in each group to vary. In this case, this means that the relationship between the final price and the length of the auction is moderated by the condition of each item.

Interaction models are easy to visualize in the data space with `ggplot2` because they have the same coefficients as if the models were fit independently to each group defined by the level of the categorical variable. In this case, new and used MarioKarts each get their own regression line. To see this, we can set an aesthetic (e.g. `color`) to the categorical variable, and then add a `geom_smooth()` layer to overlay the regression line for each color.

**Instructions**

The dataset `mario_kart` is already loaded in your workspace.

- Use the `color` aesthetic and the `geom_smooth()` function to plot the interaction model between duration and condition in the data space. Make sure you set the `method` and `se` arguments of `geom_smooth()`.

```{r}
# interaction plot
ggplot(data = mario_kart, aes(x = duration, y = totalPr, color = cond)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = 0)
```

## **Simpson's Paradox**

**1. Simpson's Paradox**

**2. SAT scores and teacher salary**

It seems reasonable to think that by paying their teachers a higher salary, schools could attract better teachers, which would lead to better student outcomes. Yet if we fit a simple linear regression model for the average SAT score among students as a function of average teacher salary across all US states, we see a negative slope. This suggests that states that pay higher teacher salaries---on average---are associated with lower student performance on the SAT. What gives?

```{r}
ggplot(data = SAT, aes(x = salary, y = total)) +
  geom_point() + 
  geom_smooth(method = 'lm', se = 0)
```

**3. Percentage taking the SAT**

It turns out that the rate at which students take the SAT has a moderating effect on this relationship. Consider how things change when we add a categorical variable to make this a parallel slopes model. In this case, we want to separate states into three groups based on how many of their students take the SAT. First, we use the cut() function to add a new sat_bin variable to our data frame. Then we fit the parallel slopes model. Note that in this case the categorical variable sat_bin is not binary---it has three levels. This results in another coefficient. How many lines to you think we will see in the plot?

```{r}
SAT_wbin <- SAT %>% 
  mutate(sat_bin = cut(sat_pct, 3))
lm(total ~ salary + sat_bin, data = SAT_wbin)

ggplot(data = SAT_wbin, aes(x = salary, y = total, color = sat_bin)) + 
  geom_point() +
  geom_line(data = broom::augment(lm(total ~ salary + sat_bin, data = SAT_wbin)), aes(y = .fitted))
```

**4. Simpson's paradox**

If you guess three, you were correct. But wait, now all three lines have a positive slope! This phenomenon is known as Simpson's paradox, and it occurs widely in the social and natural sciences. When Simpson's paradox is present the direction of the relationship between two variables changes if subgroups are considered. Although the y variable may be positively associated with x within multiple groups, it may be the case that y is negatively associated with x when those groups are ignored. When Simpson's paradox occurs, the group membership is an important confounder that must be controlled for in order to build an appropriate model.

**5. Let's practice!**

We'll see an example of this in the next exercises.

### **Simpson's paradox in action**

A mild version of [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox) can be observed in the MarioKart auction data. Consider the relationship between the final auction price and the length of the auction. It seems reasonable to assume that longer auctions would result in higher prices, sinceâ€”other things being equalâ€”a longer auction gives more bidders more time to see the auction and bid on the item.

However, a simple linear regression model reveals the opposite: longer auctions are associated with lower final prices. The problem is that all other things are not equal. In this case, the new MarioKartsâ€”which people pay a premium forâ€”were mostly sold in one-day auctions, while a plurality of the used MarioKarts were sold in the standard seven-day auctions.

Our simple linear regression model is misleading, in that it suggests a negative relationship between final auction price and duration. However, *for the used* MarioKarts, the relationship is positive.

**Instructions**

The object `slr` is already defined for you.

- Fit a simple linear regression model for final auction price (totalPr) as a function of duration (`duration` ).
- Use `aes()` to add a color aesthetic that's mapped to the condition variable to the `slr` object, which is the plot shown at right.

```{r}
slr <- ggplot(mario_kart, aes(y = totalPr, x = duration)) +   geom_point() + 
  geom_smooth(method = "lm", se = FALSE)

# model with one slope
lm(totalPr ~ duration, data = mario_kart)

# plot with two slopes
slr + aes(color = cond)
```

