---
title: "04 - Interpreting regression models"
output: html_notebook
---

```{r, echo = FALSE, message = FALSE}
library(openintro)
library(Stat2Data)
library(ggplot2)
library(dplyr)
data('textbooks')
colnames(textbooks) <- c('deptAbbr', 'course', 'isbn', 'uclaNew', 'amazNew', 'more', 'diff')
data('possum')
data("ncbirths")
data('mammals')
colnames(mammals) <- c("Species", "BodyWt", "BrainWt", "NonDreaming", "Dreaming","TotalSleep", "LifeSpan", "Gestation", "Predation", "Exposure", "Danger")
data('bdims')
data('smoking')
mlbBat10 <- mlbbat10
Anscombe <- data.frame(x = c(anscombe$x1, anscombe$x2, anscombe$x3, anscombe$x4), y = c(anscombe$y1, anscombe$y2, anscombe$y3, anscombe$y4), set = rep(c(1, 2, 3, 4), each = 11))
colnames(mlbBat10) <- c("name", "team", "position", "G", "AB", "R", "H", "2B", "3B", "HR", "RBI", "TB", "BB", "SO", "SB", "CS", "OBP", "SLG", "AVG")
noise <- data.frame(x = rnorm(1000), y = rnorm(1000), z = rep(1:20, 50))
galton <- read.csv('/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-R/Datasets/galton.csv')
```

## **Interpretation of Regression**

**1. Interpretation of Regression**

We've spent some time building our graphical intuition about regression as well as learning the mathematical specification of the model. Now it's time to focus on what is often the most important thing about a regression model: interpreting the value of the coefficients.

**2. Is that textbook overpriced?**

Data were collected on 73 textbooks required for a few randomly selected courses at UCLA. For each textbook, we know the retail price at both the UCLA bookstore and on Amazon dot com. We also know the department and course number for each corresponding course and the ISBN of the book. What factors are associated with the price of each book at the UCLA bookstore? One might suppose that more advanced books cost more.

```{r}
head(textbooks)
```

**3. Compared to the course number?**

Our best guess for the level of the course is to extract the course number. The scatter plot shows the relationship between the course number and the price of the book at the UCLA bookstore. This relationship is very weak and, if anything, it appears to be negative. Instead, since Amazon dot com provides a ready alternative for UCLA students,

```{r}
textbooks %>% mutate(course_number = readr::parse_number(as.character(course))) %>% 
  ggplot(aes(x = course_number, y = uclaNew)) +
  geom_point()
```

**4. Compared to Amazon?**

let's consider the relationship between the prices of these books at Amazon, relative to their price at the UCLA bookstore. Here we see clear evidence of a strong, positive, linear relationship. As noted previously,

```{r}
textbooks %>% 
  ggplot(aes(x = amazNew, y = uclaNew)) +
  geom_point()
```

**5. Compared to Amazon?**

the regression line can be added to the plot with the geom smooth command. While this provides us with the a way to visualize our model, it doesn't actually tell us what the fitted coefficients are.

```{r}
textbooks %>% 
  ggplot(aes(x = amazNew, y = uclaNew)) +
  geom_point() + 
  geom_smooth(method = 'lm', se = FALSE)
```

**6. Slope and intercept**

To get those values, we'll use the lm command to actually fit the model. We specify two arguments to lm, a formula that indicates which variable is the response and which is the explanatory. And a data argument that identifies the data frame where those variables are located. The output from lm reminds us how we called it and then gives us the fitted coefficients. The value of the intercept coefficient beta naught hat is 0 (point) 929 dollars, while the value of the slope coefficient beta one hat is one dollar and twenty cents. What this says is that **for each additional dollar that Amazon charges for a book, the UCLA bookstore charges about one dollar and twenty cents**. In effect, the prices at the UCLA bookstore are about 20% higher than those on Amazon. Generally, we are most interested in the slope coefficient, but occasionally the intercept coefficient is interesting as well. In this case, the intercept is not interesting: it merely tells us that we should expect a book that retails for 0 dollars on Amazon to cost about 93 cents at the UCLA bookstore. This, of course, is irrelevant. Furthermore, since the least expensive book present in the dataset cost eight dollars and sixty cents, to discuss a book that was free on Amazon would be to extrapolate our findings to values outside the range of the data we collected. This is always dangerous.

```{r}
lm(uclaNew ~amazNew, data = textbooks)
```

**7. Units and scale**

When interpreting slope coefficients, one must pay careful attention to units and scales. Note that the units of the slope coefficient is the units of the response variable per unit of the explanatory variable. In this case, the prices at both bookstores are in dollars, but that is easily changed. Here, we create a new variable for the price at Amazon in cents, and refit the model. Note that while the coefficient has changed, the underlying meaning has not. Here, we say that for each additional cent that a book costs on Amazon, the expected price at the UCLA bookstore increases by about 0 (point) 01199 dollars, or 1 (point) 2 cents. Thus, in both cases, the price of a book at the UCLA bookstore is about 20% higher, on average, than the corresponding price on Amazon.com.

```{r}
textbooks %>% 
  mutate(amazNew_cents = amazNew * 100) %>% 
  lm(uclaNew ~ amazNew_cents, data = .)
```

**8. Let's practice!**

It's time for you to start fitting your own regression models.

### **Fitting simple linear models**

While the `geom_smooth(method = "lm")` function is useful for drawing linear models on a scatterplot, it doesn't actually return the characteristics of the model. As suggested by that syntax, however, the function that creates linear models is `lm()`. This function generally takes two arguments:

- A `formula` that specifies the model
- A data argument for the data frame that contains the data you want to use to fit the model

The `lm()` function return a model object having class `"lm"`. This object contains lots of information about your regression model, including the data used to fit the model, the specification of the model, the fitted values and residuals, etc.

**Instructions**

- Using the `bdims` dataset, create a linear model for the weight of people as a function of their height.
- Using the `mlbBat10` dataset, create a linear model for `SLG` as a function of `OBP`.
- Using the `mammals` dataset, create a linear model for the body weight of mammals as a function of their brain weight, after taking the natural log of both variables.

```{r}
# Linear model for weight as a function of height
lm(wgt ~ hgt, data = bdims)

# Linear model for SLG as a function of OBP
lm(SLG ~ OBP, data = mlbBat10)

# Log-linear model for body weight as a function of brain weight
lm(formula = log(BodyWt) ~ log(BrainWt), data = mammals)
0.009+1.11*
```

## **Your linear model object**

**1. Your linear model object**

In the previous video we learned how to fit a regression model using the lm command. However, we didn't do much with it; we only displayed the fitted coefficients in the console. The output from lm is an object and there are a lot of useful things that you can do with that object.

**2. Is that textbook overpriced?**

To get started, we need to store the output from lm as an object in our environment, in this case aptly named mod. Note that mod is of class lm. This object contains all of the information needed to fit our regression model, including, by default, the relevant data and lots of other pieces of information that we can extract in various ways. It's worth repeating that mod is an object of type lm it's not a data frame, a function, a matrix, or a list.

```{r}
mod <- lm(uclaNew ~ amazNew, data = textbooks)
class(mod)
```

**3. Print**

By default, when you try to print an lm object, you see the call, as well as the fitted coefficients.

```{r}
mod
```

**4. Fitted coefficients**

You can also return just the fitted coefficients as a vector using the coef function. For our purposes, these are the pieces of information that we are most interested in. In this course, we treat regression as a descriptive statistical technique thus explaining our focus on the coefficients. In a later course, you will learn about inference for regression. In that application, there is a whole host of other pieces of information about your regression model that you'll want to inspect.

```{r}
coef(mod)
```

**5. Summary**

The summary function displays these. Just about every statistical software package has a function that displays a similar table of outputs for a regression model. However, we won't dive into this at the moment. Since the object mod contains everything R knows about our model, we can ask R for the fitted values, using

```{r}
summary(mod)
```

**6. Fitted values**

the wait for it fitted values function. This returns a vector containing the y hat values for each data point. In general, the length of the vector of fitted values is the same as the number of rows in the original data frame, since each observation corresponds to exactly one value of y hat. However, if there were any observations with missing data, those will be automatically discarded by R when the model is fit, and thus, the length of the vector of fitted values may not be a large as the number of rows in the original data frame.

```{r}
fitted.values(mod)
```

**7. Residuals**

Similarly, each fitted value generates a residual. This residual is the difference between the actual observed value of the response variable and the expected value of the response according to our model. These residuals can be retrieved using the residuals function, which returns the vectors of residuals. The R ecosystem is constantly evolving. In this course and series of courses, we have been working with a set of tools called the tidyverse.

```{r}
residuals(mod)
```

**8. broom**

One of the packages in the tidyverse is called broom, since its goal is to help you tidy up a bit. By loading the broom package and then running the augment function on our model object, we recover a data frame that contains our original response and explanatory variable, along with the fitted values, residuals, leverage scores, and several other pieces of information relevant to each observation. Working with these tidy data frames can simplify some of the work we do with our models after they are fit.

```{r}
library(broom)
augment(mod)
```

**9. Let's practice!**

In the next series of exercises, you'll work with your own linear model objects.

### **The lm summary output**

An `"lm"` object contains a host of information about the regression model that you fit. There are various ways of extracting different pieces of information.

The `coef()` function displays only the values of the coefficients. Conversely, the `summary()` function displays not only that information, but a bunch of other information, including the associated standard error and p-value for each coefficient, the $R^{2}$, adjusted $R^{2}$, and the residual standard error. The summary of an `"lm"` object in R is very similar to the output you would see in other statistical computing environments (e.g. Stata, SPSS, etc.)

**Instructions**

We have already created the `mod` object, a linear model for the weight of individuals as a function of their height, using the `bdims` dataset and the code

```{r}
mod <- lm(wgt ~ hgt, data = bdims)
```

Now, you will:

- Use `coef()` to display the coefficients of `mod`.
Use `summary()` to display the full regression output of `mod`.

```{r}
# Show the coefficients
coef(mod)

# Show the full output
summary(mod)
```

### **Fitted values and residuals**

Once you have fit a regression model, you are often interested in the fitted values ($\hat{y}$) and the residuals ($e_{i}$)), where $i$ indexes the observations. Recall that:
$$e_{i} = y_{i} - \hat{y_{i}}$$
The least squares fitting procedure guarantees that the mean of the residuals is zero (n.b., numerical instability may result in the computed values not being *exactly* zero). At the same time, the mean of the fitted values must equal the mean of the response variable.

In this exercise, we will confirm these two mathematical facts by accessing the fitted values and residuals with the `fitted.values()` and `residuals()` functions, respectively, for the following model:

```{r}
mod <- lm(wgt ~ hgt, data = bdims)
```

**Instructions**

`mod` (defined above) is available in your workspace.

- Confirm that the mean of the body weights equals the mean of the fitted values of `mod`.
- Compute the mean of the residuals of `mod`.

```{r}
# Mean of weights equal to mean of fitted values?
mean(bdims$wgt) == mean(fitted.values(mod))

# Mean of the residuals
round(mean(residuals(mod))) == 0
```

### **Tidying your linear model**

As you fit a regression model, there are some quantities (e.g. $R^{2}$) that apply to the model as a whole, while others apply to each observation (e.g. $\hat{y_{i}}$). If there are several of these per-observation quantities, it is sometimes convenient to attach them to the original data as new variables.

The `augment()` function from the `broom` package does exactly this. It takes a model object as an argument and returns a data frame that contains the data on which the model was fit, along with several quantities specific to the regression model, including the fitted values, residuals, leverage scores, and standardized residuals.

**Instructions**

The same linear model from the last exercise, `mod`, is available in your workspace.

- Load the `broom` package.
_ Create a new data frame called `bdims_tidy` that is the augmentation of the `mod` linear model.
- View the `bdims_tidy` data frame using `glimpse()`.

```{r}
# Load broom
library(broom)

# Create bdims_tidy
bdims_tidy <- augment(mod)

# Glimpse the resulting data frame
glimpse(bdims_tidy)
```

## **Using your linear model**

**1. Using your linear model**

Recall our previous example about textbooks at the UCLA bookstore.

**2. Is that textbook overpriced?**

We fit the regression model using the lm command, and stored the resulting model object.

```{r}
mod <- lm(uclaNew ~ amazNew, data = textbooks)
```

**3. Examining residuals**

By examining the residuals, we can learn about whether particular textbooks appear to be under or overpriced. In this case, the most overpriced book cost $197 at the UCLA bookstore, but just $131 on Amazon, a markup of $66! The model predicts a cost of $158 resulting in a residual of $39.

```{r}
library(broom)
augment(mod) %>% 
  arrange(desc(.resid)) %>% 
  head()
```

**4. Markup**

This turns out to be the management textbook "Financial Statement Analysis and Security Valuation". I suppose this qualifies as irony. What about textbooks that aren't in our original dataset?

```{r}
textbooks %>% 
  filter(uclaNew == 197)
```

**5. Making predictions**

Using our model to make predictions about new observations so-called "out-of-sample" observations is a powerful technique fundamental in machine learning. For example, the OpenIntro book "Introductory Statistics with Randomization and Simulation" sells for eight dollars and forty-nine cents on Amazon. What would our model predict is the retail price at the UCLA bookstore? 

**6. Making predictions**

The predict function,

**7. Making predictions**

when applied to an lm object,

**8. Making predictions**

will return the fitted values for the original observations by default. However,

**9. Making predictions**

if we specify the newdata argument,

**10. Making predictions**

we can use the model to make predictions about any observations we want. Note that the object passed to newdata must be a data frame with a variable with the same name as the explanatory variable used to fit the model. Note that the result is a vector of fitted values.

**11. New data**

Here, we create a simple data frame with one variable and one observation for the ISRS book. The model returns that the expected price at the UCLA bookstore is eleven dollars and eleven cents. I don't actually know what it sells for at UCLA, but at Smith it is selling for eleven dollars and forty cents, a slightly steeper markup.

```{r}
new_data <- data.frame(amazNew = 8.49) 
predict(mod, newdata = new_data)
```

**12. Visualize new observations**

Alternatively, the augment function from broom will also take a newdata argument. However, this function will return a data frame. This is useful if you want to do a bit more with your predictions. Here, we first use augment to create a new data frame of predicted values and then, use geom point to put those observations on the scatter plot of the original data.

```{r}
isrs  <-  broom::augment(mod, newdata = new_data)
ggplot(data = textbooks, aes(x = amazNew, y = uclaNew)) +
  geom_point() + 
  geom_smooth(method = 'lm') +
  geom_point(data = isrs, aes(y= .fitted), size = 3, color = 'red')

```

**13. Visualize new observations**

Here the single observation for the ISRS book is shown in red.

**14. Let's practice!**
Now it's time for you to make some of your own predictions.

### **Making predictions**

The `fitted.values()` function or the `augment()`-ed data frame provides us with the fitted values for the observations that were in the original data. However, once we have fit the model, we may want to compute expected values for observations that were **not** present in the data on which the model was fit. These types of predictions are called *out-of-sample*.

The `ben` data frame contains a height and weight observation for one person. The `mod` object contains the fitted model for weight as a function of height for the observations in the `bdims` dataset. We can use the `predict()` function to generate expected values for the weight of new individuals. We must pass the data frame of new observations through the `newdata` argument.

**Instructions**

The same linear model, `mod`, is defined in your workspace.

- Print `ben` to the console.
- Use `predict()` with the `newdata` argument to compute the expected weight of the individual in the `ben` data frame.

```{r, echo = FALSE}
ben <- data.frame(wgt = 74.8, hgt = 182.8)
mod <- lm(wgt ~ hgt, data = bdims)
```

```{r}
# Print ben
ben

# Predict the weight of ben
predict(mod, newdata = ben)
```

Note that the data frame ben has variables with the exact same names as those in the fitted model.

### **Adding a regression line to a plot manually**

The `geom_smooth()` function makes it easy to add a simple linear regression line to a scatterplot of the corresponding variables. And in fact, there are more complicated regression models that can be visualized in the data space with `geom_smooth()`. However, there may still be times when we will want to add regression lines to our scatterplot manually. To do this, we will use the `geom_abline()` function, which takes `slope` and `intercept` arguments. Naturally, we have to compute those values ahead of time, but we already saw how to do this (e.g. using `coef()`).

The `coefs` data frame contains the model estimates retrieved from `coef()`. Passing this to `geom_abline()` as the data argument will enable you to draw a straight line on your scatterplot.

**Instructions**

Use `geom_abline()` to add a line defined in the `coefs` data frame to a scatterplot of weight vs. height for individuals in the `bdims` dataset.

```{r, echo = FALSE}
coefs <- tibble(`(Intercept)` = coef(mod)[1], hgt = coef(mod)[2])
```

```{r}
# Add the line to the scatterplot
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_abline(data = coefs, 
              aes(intercept = `(Intercept)`, slope = hgt),  
              color = "dodgerblue")
```

