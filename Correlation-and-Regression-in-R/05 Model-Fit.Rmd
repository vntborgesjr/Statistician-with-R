---
title: "05 - Model Fit"
output: html_notebook
---

```{r, echo = FALSE, message = FALSE}
library(openintro)
library(Stat2Data)
library(ggplot2)
library(dplyr)
data('textbooks')
colnames(textbooks) <- c('deptAbbr', 'course', 'isbn', 'uclaNew', 'amazNew', 'more', 'diff')
data('possum')
colnames(possum) <- c('site', 'pop', 'sex', 'age', 'headL',
                      'skullW', 'totalL', 'tailL')
data("ncbirths")
data('mammals')
colnames(mammals) <- c("Species", "BodyWt", "BrainWt", "NonDreaming", "Dreaming","TotalSleep", "LifeSpan", "Gestation", "Predation", "Exposure", "Danger")
data('bdims')
data('smoking')
mlbBat10 <- mlbbat10
Anscombe <- data.frame(x = c(anscombe$x1, anscombe$x2, anscombe$x3, anscombe$x4), y = c(anscombe$y1, anscombe$y2, anscombe$y3, anscombe$y4), set = rep(c(1, 2, 3, 4), each = 11))
colnames(mlbBat10) <- c("name", "team", "position", "G", "AB", "R", "H", "2B", "3B", "HR", "RBI", "TB", "BB", "SO", "SB", "CS", "OBP", "SLG", "AVG")
noise <- data.frame(x = rnorm(1000), y = rnorm(1000), z = rep(1:20, 50))
galton <- read.csv('/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-R/Datasets/galton.csv')
```

## **Assessing Model Fit**

**1. Assessing Model Fit**

Now that we understand what linear regression models are and how they work, a natural next question is to consider how well they work.

**2. How well does our textbook model fit?**

In an intuitive sense, it seems clear that the regression line for the textbooks fits really well.

```{r}
ggplot(data = textbooks, aes(x = amazNew, y = uclaNew)) +
  geom_point() + 
  geom_smooth(method = 'lm', se = FALSE)
```

**3. How well does our possum model fit?**

At the same time, the regression line for the possums fits less well, but it still seems useful. Can we quantify our intuition about the quality of the model fit?

```{r}
ggplot(data = possum, aes(y = totalL, x = tailL)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```

**4. Sums of squared deviations**

In fact, we can. Recall that we initially considered any number of lines. We settled on the unique regression line by applying the least squares criterion. That is, we found the line that minimizes the sum of the squared residuals. For each observation, which is represented on the scatter plot by a point, the residual is simply the vertical distance between that point and the line. Here, we have highlighted the possum residuals with grey arrows. If we could find a line that made those gray arrows shorter, collectively and after squaring them, that would be our regression line. But there is no such line. This one is the best. Note that we can't just minimize the sum of the residuals. That number is always zero, since the positive and negative residuals cancel each other out when added together.

```{r}
mod <- lm(totalL ~ tailL, possum)
ggplot(data = possum, aes(y = totalL, x = tailL)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE) +
  geom_segment(aes(x = tailL, xend = tailL, y = fitted.values(mod), yend = totalL), arrow = arrow(ends = 'first', type = 'closed', length = unit(0.1, 'cm')), color = "grey") 
```

**5. SSE**

The sum of the squares works well mathematically, but it also has the effect of penalizing large residuals disproportionately. This is generally considered a useful property for statistical modeling, since you would usually prefer a model that misses often by a little bit, but never by a lot to a model that works well most of the time, but occasionally is way off. Once again, there are situations when other criteria are used for fitting models, but we won't talk about them in this course. After using the augment function to tidy up our model, the sum of the squared residuals is easily computed using summarize. By convention, we often call this quantity SSE, for sum of squared errors. It can also be computed as the variance of the residuals times one fewer than the number of observations. The SSE is a single number that captures how much our model missed by. Unfortunately, it is hard to interpret, since the units have been squared.

```{r}
library(broom)
mod_possum <- lm(totalL ~ tailL, data = possum)
mod_possum %>% 
  augment() %>% 
  summarize(SSE = sum(.resid^2),
            SSE_also = (n() - 1) * var(.resid))
```

**6. RMSE**

Thus, another common way of thinking about the accuracy of a model is the root mean squared error, or RMSE. The RMSE is essentially the standard deviation of the residuals. You might expect us to divide by n here, but we instead divide by the number of degrees of freedom, which in this case is n-2. The concept of degrees of freedom comes up in many contexts in statistics, but a fuller discussion is beyond the scope of this course. The RMSE also generalizes to any kind model for a single numerical response, so it is not specific to regression models.

**7. Residual standard error (possums)**

When R displays the summary of a regression model, it displays the "residual standard error". This is the RMSE. Conveniently, the RMSE is in the units of the response, so this says that our model makes a predicted body length that is typically within about 3 (point) 57 centimeters of the truth. That seems useful, since the possums in our dataset are between 75 and 96 centimeters.

```{r}
summary(mod_possum)
```

**8. Residual standard error (textbooks)**

For the textbooks, the residual standard error is ten dollars and forty-seven cents. Somehow, this doesn't seem as useful and yet it seemed from the scatter plot that the fit of the textbook model was much better than the fit of the possum model. Reconciling these two notions will be the subject of our next video.

```{r}
summary(lm(uclaNew ~ amazNew, data = textbooks))
```

**9. Let's practice!**

You'll work with residuals on your own in the next exercises.

### **Standard error of residuals**

One way to assess strength of fit is to consider how far off the model is for a typical case. That is, for some observations, the fitted value will be very close to the actual value, while for others it will not. The magnitude of a typical residual can give us a sense of generally how close our estimates are.

However, recall that some of the residuals are positive, while others are negative. In fact, it is guaranteed by the least squares fitting procedure that the mean of the residuals is zero. Thus, it makes more sense to compute the square root of the mean squared residual, or *root mean squared error* ($RMSE$). R calls this quantity the *residual standard error*.

To make this estimate unbiased, you have to divide the sum of the squared residuals by the degrees of freedom in the model. Thus,

 $$RMSE = \sqrt{\frac {\sum_{i} e_{i}^{2}}{d.f.}} = \sqrt{\frac{SSE}{d.f.}}$$
 
You can recover the residuals from `mod` with `residuals()`, and the degrees of freedom with `df.residual()`.

**Instructions**

- View a `summary()` of `mod`.
- Compute the mean of the `residuals()` and verify that it is approximately zero.
- Use `residuals()` and `df.residual()` to compute the root mean squared error ($RMSE$), a.k.a. *residual standard error*.

```{r, echo = FALSE}
mod <- lm(wgt ~ hgt, data = bdims)
```

```{r}
# View summary of model
summary(mod)

# Compute the mean of the residuals
mean(residuals(mod))

# Compute RMSE
sqrt(sum(residuals(mod)^2) / df.residual(mod))
```

## **Comparing model fits**

**1. Comparing model fits**

In the previous video, you learned about how we could use the sum of the squared residuals to quantify how well our model fit the data.

**2. How well does our textbook model fit?**

However, we noted that although the textbook model seemed to fit the data really well, the residual standard error was more than $10.

```{r}
ggplot(data = textbooks, aes(x = amazNew, y = uclaNew)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = FALSE)
```

**3. How well does our possum model fit?**

On the other hand, the residual standard error for the possum model was about three and half centimeters, which seems like a high degree of accuracy for a model that does seem to be as tight of a fit. Moreover, it's hard to compare $10 to three and half centimeters. Which is bigger? What would be nice is if we had a way to compare the quality of the fit that was unitless. To do so, it is helpful to think about a benchmark.

```{r}
possum %>% 
  ggplot(aes(y = totalL, x = tailL)) +
  geom_point() + 
  geom_smooth(method = 'lm', se = FALSE)
```

**4. Null (average) model**

If you had to predict the body length of a possum and you didn't have any information about that particular possum, what would your prediction be? I'll pause while you think. A sensible choice would be the average length of all possums. And in fact, if you have to make the same prediction for every possum, the average is the best number you can pick. We can think about this as a model where y hat, the predicted value of y, is equal to y bar, the average value of y. This model is often called the null model. This model makes sense to use as a benchmark, since it doesn't require any insight to make and yet there is no reasonable model that could be any worse.

**5. Visualization of null model**

It looks like this. We can fit the null model in R using lm, but including only the constant 1 as our explanatory variable.

```{r}
possum %>% 
  ggplot(aes(x = tailL, y = totalL)) +
  geom_point() +
  geom_line(aes(x = tailL, y = mean(totalL)), color = 'blue') +
  geom_segment(aes(x = tailL, xend = tailL, y = predict(lm(totalL ~ 1)), yend = totalL), arrow = arrow(ends = 'first', type = 'closed', length = unit(0.1, 'cm')), color = "grey")
```

**6. SSE, null model**

This results in an SSE value of 1913. 

```{r}
mod_null <- lm(totalL ~ 1, data = possum)
mod_null %>% 
  augment(possum) %>% 
  summarize(SSE = round(sum(.resid^2)))
```

**7. SSE, our model**

Compare this number to the SSE for our possum model that uses tail length as an explanatory variable. The SSE in this case is 1301. The ratio of the SSE for our model to the SSE for the null model is a quantification of the variability explained by our model.

```{r}
mod_null <- lm(totalL ~ tailL, data = possum)
mod_null %>% 
  augment() %>% 
  summarize(SSE = round(sum(.resid^2)))

```

**8. Coefficient of determination**

More specifically, the SSE for the null model is often called SST, for the total sum of the squares. This is a measure of the variability in the response variable. By building a regression model, we hope to explain some of that variability. The portion of the SST that is not explained by our model is the SSE. These ideas are captured by this formula for the coefficient of determination, usually referred to as R squared. Due to this definition, we interpret R squared as the proportion of the variability in the response variable that is explained by our model. It is the most commonly cited measure of the quality of the fit of a regression model.

**9. Connection to correlation**

We have already seen a connection between the value of the correlation between x and y and the slope of the regression line. In fact, the value of the correlation coefficient is also closely related to the value of R squared. For least squares regression models with a single explanatory variable, the value of R squared is just the square of the correlation coefficient.

**10. Summary**

Why then, do we need both concepts? Correlation is strictly a bivariate quantity it can only be between a single response and a single explanatory variable. However, regression is a much more flexible modeling framework. Each regression model has its own value of R squared, but in future chapters you will learn how such models can incorporate many explanatory variables, unlike correlation. The easiest way to see the R squared value is to apply the summary function to your model object. In this case, we see that our model based on tail length explains about 32% of the variability in body length for these possums. While R squared is certainly a useful and ubiquitous measure of model fit, it is not the be-all-and-end-all of statistical modeling.

```{r}
summary(mod_possum)
```

**11. Over-reliance on R-squared**

A high R squared alone doesn't mean that you have a good model and low R squared doesn't mean that you have a lousy model. A model with a high R squared may be overfit or it may violate the conditions for inference that we will discuss in a later chapter. A model with a low R squared can still provide statistically significant insight into a complex problem. We'll close by invoking the words of famed statistician George Box: "Essentially, all models are wrong, but some are useful".

**12. Let's practice!**

Now it's time for you to assess model fit on your own.

### **Assessing simple linear model fit**

Recall that the coefficient of determination ($R^{2}$), can be computed as
 
 $$R^{2} = 1 - \frac{SSE}{SST} = 1 - \frac{Var(e)}{Var(y)},$$
where $e$ is the vector of residuals and $y$ is the response variable. This gives us the interpretation of $R^{2}$ as the percentage of the variability in the response that is explained by the model, since the residuals are the part of that variability that remains unexplained by the model.

**Instructions**

The `bdims_tidy` data frame is the result of `augment()`-ing the `bdims` data frame with the `mod` for `wgt` as a function of `hgt`.

- Use the `summary()` function to view the full results of `mod`.
- Use the `bdims_tidy` data frame to compute the $R^{2}$
 of `mod` manually using the formula above, by computing the ratio of the variance of the residuals to the variance of the response variable.

```{r, echo = FALSE}
mod <- lm(wgt ~ hgt, data = bdims)
bdims_tidy <- augment(mod)
```

```{r}
# View model summary
summary(mod)

# Compute R-squared
bdims_tidy %>%
  summarize(var_y = var(wgt), var_e = var(.resid)) %>%
  mutate(R_squared = 1 - var_e/var_y)
```

This means that 51.4% of the variability in weight is explained by height.

### **Linear vs. average**

The $R^{2}$ gives us a numerical measurement of the strength of fit relative to a null model based on the average of the response variable:
$$\hat{y}_{null} = \overline{y}$$
This model has an $R^{2}$ of zero because $SSE = SST$. That is, since the fitted values ($\hat{y}_{null}$) are all equal to the average ($\overline{y}$), the residual for each observation is the distance between that observation and the mean of the response. Since we can always fit the null model, it serves as a baseline against which all other models will be compared.

In the graphic, we visualize the residuals for the null model (`mod_null` at left) vs. the simple linear regression model (`mod_hgt` at right) with height as a single explanatory variable. Try to convince yourself that, if you squared the lengths of the grey arrows on the left and summed them up, you would get a larger value than if you performed the same operation on the grey arrows on the right.

```{r}
library(patchwork)
bdims %>% 
  ggplot(aes(x = hgt, y = wgt)) +
  geom_point(color = 'blue', size = 0.5) +
  geom_smooth(aes(x = hgt, y = mean(wgt)), method = 'lm', se = FALSE) +
  geom_segment(aes(x = hgt, xend = hgt, y = mean(wgt), yend = wgt), arrow = arrow(length = unit(0.01, 'cm'), ends = 'first', type = 'closed'), size = 0.5, alpha = 0.3, color = 'grey') +
  labs(title = '            Null') + 
  theme(title = element_text(face = 'bold')) | 
bdims %>%
  ggplot(aes(x = hgt, y = wgt)) +
  geom_smooth(method = 'lm', se = FALSE) +
  geom_point(color = 'blue', size = 0.5) +
  geom_segment(aes(x = hgt, xend = hgt, y = fitted.values(lm(wgt ~ hgt)), yend = wgt), arrow = arrow(length = unit(0.01, 'cm'), ends = 'first', type = 'closed'), color = 'grey', size = 0.5, alpha = 0.3) +
  labs(title = '              hgt') + 
  theme(title = element_text(face = 'bold'))


```

It may be useful to preview these `augment()`-ed data frames with `glimpse`():

```{r, echo = FALSE}
mod_null <- augment(lm(wgt ~ 1, data = bdims))
mod_hgt <- augment(lm(wgt ~ hgt, data = bdims))
```

```{r}
glimpse(mod_null)
glimpse(mod_hgt)
```

**Instructions**

- Compute the sum of the squared residuals ($SSE$) for the null model `mod_null`.
- Compute the sum of the squared residuals ($SSE$) for the regression model `mod_hgt`.

```{r}
# Compute SSE for null model
mod_null %>%
  summarize(SSE = sum(.resid^2))

# Compute SSE for regression model
mod_hgt %>%
  summarize(SSE = sum(.resid^2))
```

## **Unusual Points**

**1. Unusual Points**

In our previous discussion of outliers, we learned how to identify points that seem to be unusual. In this video, we will refine that understanding by introducing two related but distinct concepts: leverage and influence.

**2. Unusual points**

Recall the data we examined previously about Major League Baseball players during the 2010 season. We considered the relationship between the number of home runs hit by each player and the corresponding number of bases that each player stole. The first statistic is a measurement of power, while the latter is a measurement of speed. As these skills are considered complementary, it should not be surprising that a simple linear regression model has a negative slope. In this case, we have fit the model to only those players with at least 400 at-bats, in a simple attempt to control for the confounding influence of playing time. We noted previously that there were two potential outliers here:

```{r}
regulars <- mlbBat10 %>% 
  filter(AB > 400) 
ggplot(data = regulars, aes(x = SB, y = HR)) +
  geom_point() +
  geom_smooth(method  = 'lm', se = FALSE)
```

**3. Unusual points**

the point corresponding to the slugger Jose Bautista in the upper left and the point belonging to speedster Juan Pierre in the lower right. Now that we have a regression model, we want to think about how individual observations might affect the slope of the line.

```{r}
regulars <- mlbBat10 %>% 
  filter(AB > 400) 
ggplot(data = regulars, aes(x = SB, y = HR)) +
  geom_point() +
  geom_smooth(method  = 'lm', se = FALSE) +
  annotate(geom = 'point', x = c(max(regulars$SB), 9), y = c(1, max(regulars$HR)), pch = 15, size = 6, color = 'blue', alpha = 0.3) + 
  annotate(geom = 'text', x = c(max(regulars$SB) - 5, 9 + 10), y = c(1 + 6, max(regulars$HR)), label = c('Juan Pierre', 'Jose Batista'), color = 'blue')
```

**5. Unusual points**

Typically, the purpose of interpreting the slope coefficient is to learn about the overall relationship between the two variables, so it doesn't necessarily make sense if one or two individual observations have a disproportionate effect on that slope.

**6. Leverage**

The concepts of leverage and influence will help us quantify this intuition. Leverage has a precise mathematical definition that you can see here. The specifics of the formula are not so important, but you should recognize that the leverage score h sub i for an observation is entirely a function of the distance between the value of the explanatory variable and mean of the explanatory variable. 
$$h_{i} = \frac{1}{n} + \frac{(x_{i} - \overline{x})^2}{\Sigma^{n}_{i = 1}(x_{i} - {\overline{x}})^2}$$ 
This means that points that are close to the horizontal center of the scatter plot have low leverage, while points that are far from the horizontal center of the scatter plot have high leverage. The y coordinate doesn't matter at all.

**7. Leverage computations**

It should not be surprising then that the player with the largest leverage value is the aforementioned Juan Pierre. The leverage scores can be retrieved using the augment function and then examining the hat variable. The name comes from the historical convention of computing leverage from the "hat" matrix. Note that the leverage scores depend only on stolen bases.

**8. Leverage computations**

In this case, Pierre's leverage score is nearly twice as large as that of the next player. Observations of high leverage, by virtue of their extreme values of the explanatory variable, may or may not have a considerable effect on the slope of the regression line. An observation that does have such an effect is called influential. In our case, the regression line is very close to the point corresponding to Juan Pierre anyway. So, even though this is a high leverage observation, it is not considered influential. 

```{r}
library(broom)
mod <- lm(HR ~ SB, data = regulars)
mod %>% 
  augment() %>% 
  arrange(desc(.hat)) %>% 
  select(HR, SB, .fitted, .resid, .hat) %>% 
  head()
```
**9. Consider Rickey Henderson…**

However, suppose that there was a player with a similar number of stolen bases, but a decent number of home runs as well.

**10. Consider Rickey Henderson…**

In fact, Hall of Famer Rickey Henderson was such a player and in his MVP-winning season of 1990, he stole 65 bases while hitting 28 home runs. Let's add this observation to our plot. Notice how the new regression line pulls upward ever so slightly from the previous dotted regression line. This is a direct result of Henderson's influence. Because this is a point of high leverage, it has the ability to pull the slope of the regression line up. But unlike the point corresponding to Pierre,

```{r}
regulars <- mlbBat10 %>% 
  filter(AB > 400) 
regulars2 <- rbind(regulars, list('R Henderson', NA, NA, NA, NA, NA, NA, NA, NA, 28, NA, NA, NA, NA, 65, NA, NA, NA, NA))
ggplot(data = regulars2, aes(x = SB, y = HR)) +
  geom_point() +
  geom_smooth(method  = 'lm', se = FALSE) +
  geom_smooth(data = regulars, method  = 'lm', se = FALSE,
              color = 'black', linetype = 'dotted', size = 0.5) +
  annotate(geom = 'point', x = 65, y = 28, pch = 15, size = 6, color = 'blue', alpha = 0.3) + 
  annotate(geom = 'text', x = 60, y = 32, label = 'Rickey Henderson', color = 'blue')
```

**11. Consider Rickey Henderson…**

the point corresponding to Henderson also has a large residual and the combination of high leverage and large residual determine influence.

```{r}
regulars <- mlbBat10 %>% 
  filter(AB > 400) 
regulars2 <- rbind(regulars, list('R Henderson', NA, NA, NA, NA, NA, NA, NA, NA, 28, NA, NA, NA, NA, 65, NA, NA, NA, NA))
ggplot(data = regulars2, aes(x = SB, y = HR)) +
  geom_point() +
  geom_smooth(method  = 'lm', se = FALSE) +
  geom_smooth(data = regulars, method  = 'lm', se = FALSE,
              color = 'black', linetype = 'dotted', size = 0.5) +
  annotate(geom = 'point', x = 65, y = 28, pch = 15, size = 6, color = 'blue', alpha = 0.3) + 
  annotate(geom = 'text', x = 60, y = 32, label = 'Rickey Henderson', color = 'blue') +
  annotate(geom = 'point', x = c(max(regulars$SB), 65), y = c(1, 28), pch = 15, size = 6, color = 'blue', alpha = 0.3) + 
  annotate(geom = 'text', x = c(max(regulars$SB) - 9, 60), y = c(1, 32), label = c('Juan Pierre', 'Rickey Henderson'), color = 'blue')
```

**12. Influence via Cook's distance**

In fact, a measurement known as Cook's distance combines these two quantities to measure influence. These figures are also reported by augment.

```{r}
mod <- lm(HR ~ SB, data = regulars2)
mod %>% 
  augment() %>% 
  arrange(desc(.cooksd)) %>% 
  select(HR, SB, .fitted, .resid, .hat, .cooksd)
```

**13. Influence via Cook's distance**

We note here that the observation corresponding to Henderson has a large residual, high leverage, and by far the largest value of Cook's distance.

**14. Let's practice!**

You'll explore some more outliers in these next exercises.

### **Leverage**

The *leverage* of an observation in a regression model is defined entirely in terms of the distance of that observation from the mean of the explanatory variable. That is, observations close to the mean of the explanatory variable have low leverage, while observations far from the mean of the explanatory variable have high leverage. Points of high leverage may or may not be influential.

The `augment()` function from the `broom` package will add the leverage scores (`.hat`) to a model data frame.

```{r, echo = FALSE}
mlbBat102 <- mlbBat10 %>% 
  filter(SLG < 0.9 & SLG != 0 & OBP < 0.6 & OBP != 0.5)
mod <- lm(SLG ~ OBP, data = mlbBat102)
mlbBat10 %>% 
  filter(SLG < 0.9 & SLG != 0 & OBP < 0.6 & OBP != 0.5) %>%   ggplot(aes(x = OBP, y = SLG)) + 
  geom_point() +
  ylab('SLG') + 
  xlab('OBP')
```

**Instructions**

- Use `augment()` to list the top 6 observations by their leverage scores, in descending order.

```{r}
# Rank points of high leverage
mod %>%
  augment() %>%
  arrange(desc(.hat)) %>%
  head(6)
```

### **Influence**

As noted previously, observations of high leverage may or may not be *influential*. The influence of an observation depends not only on its leverage, but also on the magnitude of its residual. Recall that while leverage only takes into account the explanatory variable ($x$), the residual depends on the response variable ($y$) and the fitted value ($\hat{y}$).

Influential points are likely to have high leverage and deviate from the general relationship between the two variables. We measure influence using Cook's distance, which incorporates both the leverage and residual of each observation.

**Instructions**

Use `augment()` to list the top 6 observations by their Cook's distance (`.cooksd`), in descending order.

```{r}
# Rank influential points
mod %>%
  augment() %>%
  arrange(desc(.cooksd)) %>%
  head(6)
```

## **Dealing with Outliers**

**1. Dealing with Outliers**

In the previous video, we learned about how leverage and influence can help us understand how outliers affect our regression model.

**2. Dealing with outliers**

Suppose you have determined that an influential observation is affecting the slope of your regression line in a way that undermines the scientific merit of your model. What can you do about it?

```{r}
ggplot(data = regulars2, aes(x = SB, y = HR)) +
  geom_point() +
  geom_smooth(method  = 'lm', se = FALSE) +
  annotate(geom = 'point', x = c(max(regulars$SB), 65, 9), y = c(1, 28, max(regulars$HR)), pch = 15, size = 6, color = 'blue', alpha = 0.3) 
```

**3. Dealing with outliers**

The short answer is that there isn't much you can do about it other than removing the outliers. As the statistical modeler, this is a decision you can make,

**4. Dealing with outliers**

but it's crucial that you understand the ramifications of this decision and act in good scientific faith. The long answer is that there are more sophisticated statistical techniques, which we won't discuss in this course, that can help you deal with the troublesome impact of outliers.

**5. The full model**

In the full model of all the regular players from 2010 and Rickey Henderson from 1990, the slope of the regression line was minus (point) 21 home runs per stolen base. In other words, players who steal five extra bases hit about one fewer home run, on average.

```{r}
coef(lm(HR ~ SB, data = regulars2))
```

**6. Removing outliers that don't fit**

Now, in this case, there is an easy argument that Rickey Henderson does not fit with the rest of these data. It's a bit of a contrived argument, since we added him previously for effect, but nonetheless there are good reasons to assert that Henderson doesn't belong. If we remove him, note how the slope of the regression line decreases. Now, it's only four extra stolen bases that are associated with hitting one fewer home run. Anytime you are thinking about removing outliers, you should ask yourself what the justification is. "Because it improves my results" is not a good justification. Indeed, conscious ignorance of valid data is not intellectually honest and has been the cause of more than a few retractions of previously published scientific papers. Be skeptical. The burden of proof is on you to make a strong argument as to why data should be omitted. Second, you must consider how this changes the scope of inference. If you are studying countries, are you omitting only the poorest countries? If so, then your results no longer apply to all countries, just non-poor countries. Misunderstanding how the scope of inference changes can be a fatal flaw in your analysis.

```{r}
coef(lm(HR ~ SB, data = regulars)) 
```

**7. Removing outliers that do fit**

With Henderson out of the way, consider removing Juan Pierre as well. Here, there really aren't any good arguments as to why this should be done. First, the point is not influential, so whether we include it or not, it won't affect our results much. More importantly, because Juan Pierre was just a regular major league player in 2010, there is no reason to think that he somehow doesn't belong to the larger group of players. What is so special about Juan Pierre that would lead us to exclude him? If, hypothetically, he was a pitcher, or he was 60 years old, or he only had one arm, then you could try and make that case. But there is nothing like that going on here, and so we have no scientific reason to exclude him.

```{r}
regulars_new <- regulars %>% 
  filter(SB < 60) # remove Pierre
  coef(lm(HR ~ SB, data = regulars_new))
```

**8. Let's practice!**

Now it's time to deal with some outliers on your own.

### **Removing outliers**

Observations can be outliers for a number of different reasons. Statisticians must always be careful—and more importantly, transparent—when dealing with outliers. Sometimes, a better model fit can be achieved by simply removing outliers and re-fitting the model. However, one must have strong justification for doing this. A desire to have a higher $R^{2}$ is not a good enough reason!

In the `mlbBat10` data, the outlier with an OBP of 0.550 is Bobby Scales, an infielder who had four hits in 13 at-bats for the Chicago Cubs. Scales also walked seven times, resulting in his unusually high OBP. The justification for removing Scales here is weak. While his performance was unusual, there is nothing to suggest that it is not a valid data point, nor is there a good reason to think that somehow we will learn more about Major League Baseball players by excluding him.

Nevertheless, we can demonstrate how removing him will affect our model.

**Instructions**

- Use `filter()` to create a subset of `mlbBat10` called `nontrivial_players` consisting of only those players with at least 10 at-bats and OBP of below 0.500.
- Fit the linear model for `SLG` as a function of `OBP` for the `nontrivial_players`. Save the result as `mod_cleaner`.
- View the `summary()` of the new model and compare the slope and $R^{2}$ to those of `mod`, the original model fit to the data on all players.
- Visualize the new model with `ggplot()` and the appropriate `geom_*()` functions.

```{r}
# Create nontrivial_players
nontrivial_players <- mlbBat10 %>% 
  filter(AB >= 10, OBP < 0.5)

# Fit model to new data
mod_cleaner <- lm(SLG ~ OBP, data = nontrivial_players)

# View model summary
summary(mod_cleaner)

# Visualize new model
nontrivial_players %>% 
  ggplot(aes(x = OBP, y = SLG)) + 
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```

### ** High leverage points**

Not all points of high leverage are influential. While the high leverage observation corresponding to Bobby Scales in the previous exercise is influential, the three observations for players with OBP and SLG values of 0 are not influential.

This is because they happen to lie right near the regression anyway. Thus, while their extremely low OBP gives them the power to exert influence over the slope of the regression line, their low SLG prevents them from using it.

**Instructions**

The linear model, `mod`, is available in your workspace. Use a combination of `augment()`, `arrange()` with two arguments, and `head()` to find the top 6 observations with the highest leverage but the lowest Cook's distance.

```{r}
# Rank high leverage points
mod %>% 
  augment() %>% 
  arrange(desc(.hat), .cooksd) %>% 
  head(6)

```

## **Conclusion**

**1. Conclusion**

This course was about analyzing the relationship between two numeric variables. We learned a variety of techniques for doing this.

**2. Graphical: scatterplots**

First, we explored how powerful scatter plots can be in revealing bivariate relationships in an intuitive, graphical form. We built a framework for describing what we see in those scatter plots and practiced implementing that framework on real data.

```{r}
possum %>% 
  ggplot(aes(x = tailL, y = totalL)) +
  geom_point() +
  ylab('Length of Possum Body (cm)') + 
  xlab('Length of Possum Tail (cm)')
```

**3. Numerical: correlation**

Second, we learned about correlation, a simple way to quantify the strength of the linear relationship between two variables using a single number. We emphasized the value of such measurements,

**4. Numerical: correlation**

but illustrated how their careless application can lead to erroneous results.

**5. Modular: linear regression**

Third, we learned about linear regression a relatively simple, yet powerful technique for modeling a response variable in terms of a single explanatory variable. We built our intuition about how these models work and identified some of their key properties.

```{r}
possum %>% 
  ggplot(aes(x = tailL, y = totalL)) +
  geom_point() +
  geom_smooth(method = 'lm', se = 0) +
  ylab('Length of Possum Body (cm)') + 
  xlab('Length of Possum Tail (cm)')
```

**6. Focus on interpretation**

Fourth, we focused carefully on how to interpret the coefficients of regression models and how those interpretations can bring real insight into complex problems.

```{r}
textbooks %>% 
  ggplot(aes(x = amazNew, y = uclaNew)) +
  geom_point() +
  geom_smooth(method = 'lm', se = 0)
```
$$\widehat{uclaNew} = 0.929 + 1.199 . amazNew$$
**7. Objects and formulas**

We also developed a foundational understanding of how to build these models in R and how to work with them afterwards.

```{r}
mod <- lm(uclaNew ~ amazNew, data = textbooks)
summary(mod)
```

**8. Model fit**

Finally, we introduced the notion of model fit and developed tools for helping us reason about the quality of our models and how much we can learn from them. Together, we hope that these concepts and techniques will inform your thinking about the nature of the relationship between variables and help to you unravel them on your own.

```{r}
possum %>% 
  ggplot(aes(x = tailL, y = totalL)) +
  geom_point() = 
  geom_smooth(method = 'lm', se = 0) +
  geom_segment(aes(x = tailL, xend = tailL, y = fitted.values(lm(totalL ~ tailL)), yend = totalL),arrow =  arrow(ends = 'first', type = 'closed', length = unit(0.1, 'cm')), size = 0.5, color = 'grey')
```

**9. Let's practice!**

It's been my pleasure to be your instructor for this course and I hope you'll continue on with our intro stats courses.