---
title: "02 - Correlation"
output: html_notebook
---

```{r, echo = FALSE, message = FALSE}
library(openintro)
library(Stat2Data)
library(ggplot2)
library(dplyr)
data('possum')
data("ncbirths")
data('mammals')
data('bdims')
data('smoking')
mlbBat10 <- mlbbat10
Anscombe <- data.frame(x = c(anscombe$x1, anscombe$x2, anscombe$x3, anscombe$x4), y = c(anscombe$y1, anscombe$y2, anscombe$y3, anscombe$y4), set = rep(c(1, 2, 3, 4), each = 11))
noise <- data.frame(x = rnorm(1000), y = rnorm(1000), z = rep(1:20, 50))
```

## **Quantifying the strength of bivariate relationships**

**1. Quantifying the strength of bivariate relationships**

In the previous chapter, we learned how to visually assess the strength of the relationship between two variables by examining a scatter plot. Correlation is a way to quantify the strength of that linear relationship.

**2. Correlation**

The correlation coefficient is a number between -1 and 1 that indicates the strength of a linear relationship. The sign of the correlation coefficient corresponds to the direction positive or negative. The magnitude of the correlation corresponds to the strength.

**3. Near perfect correlation**

A correlation coefficient of close to 1 indicates near-perfect positive correlation, as we see in this plot. For scatter plots in which the points are more spread out, the value of the correlation coefficient is lower.

```{r}

```

**4. Strong**

Here we see a scatter plot with correlation that we might call "strong".

```{r}

```

**5. Moderate**

Values of the correlation coefficient that are closer to (point) 5 might be considered "moderate".

```{r}

```

**6. Weak**

While values of the correlation coefficient closer to 0 (point) 2 might be considered "weak".

```{r}

```

**7. Zero**

If there really is no linear relationship between the two variables, then the correlation coefficient will be close to zero. This scatter plot shows two variables that are uncorrelated. Note how knowing one of the x values gives you almost no information about what the corresponding value of y will be.

```{r}

```

**8. Negative**

Of course, when the direction of the relationship is negative, the value of the correlation coefficient is as well. It is crucial to remember that the correlation coefficient only captures the strength of the linear relationship between two variables. It is quite common to encounter variables that are strongly-related, but in a nonlinear way. Blindly computing and reporting the correlation between such variables is not likely to yield meaningful results.

```{r}

```

**9. Non-linear**

This scatter plot shows a clear, quadratic relationship. To simply interpret the low correlation coefficient as evidence that x and y are unrelated would be plainly incorrect. Here, x and y are closely related just not in a linear fashion.

```{r}

```
 
**10. Non-linear correlation**

Here is an example of real data from a 10 mile run. The pace of the top ten finishers in each age group are plotted against their age. Note how the relationship between the pace and age is nonlinear: 25 year olds tend to actually be faster than 20 year olds, but as people age beyond their thirties, they tend to get slower. This pattern is present for both men and women. The value of the correlation coefficient here is about 0 (point) 68, but we have some intuitive sense that pace and age are more closely related than the figure indicates. There are several different ways that correlation can be defined in statistics,

```{r}
run10 %>% 
  filter(divPlace <= 10) %>% 
  ggplot(aes(x = age, y = pace, color = gender)) +
  geom_point()
```

**11. Pearson product-moment correlation**

but by far the most common is the Pearson Product Moment correlation. When we say correlation, we are talking about this value, but you should be aware that there are other definitions that are preferred in other contexts. Correlation is most often denoted with the letter r and it is a function of two variables, most commonly x and y. The definition in terms of covariance, which we haven't defined, and the sums of the squares is shown here.
 
**12. Pearson product-moment correlation**

This is an alternative definition using only x's and y's, and their means: x bar and y bar. In the denominator, we see the sums of the squared deviations in both x and y. In the numerator, we see a sum of terms involving both x and y. In fact, each term in the sum is the area of the rectangle with corners at each data point, x of i and y i of, and the means, x bar and y bar. So, in some sense this is measuring the deviation from the mean in both x and y. In practice, we always have R do these computations for us, so memorizing the formulas is not important. For some of you, the mathematics may help reinforce your intuition about what the correlation coefficient actually measures. For others, it may simply be a confusing blur of symbols that can be safely ignored. Having a solid conceptual understanding of what correlation is, is the important thing.

**13. Let's practice!**

You'll get some more practice computing correlation in the next set of exercises.

### **Computing correlation**

The `cor(x, y)` function will compute the Pearson product-moment correlation between variables, `x` and `y`. Since this quantity is symmetric with respect to `x` and `y`, it doesn't matter in which order you put the variables.

At the same time, the `cor()` function is very conservative when it encounters missing data (e.g. `NA`s). The use argument allows you to override the default behavior of returning `NA` whenever any of the values encountered is `NA`. Setting the use argument to `"pairwise.complete.obs"` allows `cor()` to compute the correlation coefficient for those observations where the values of `x` and `y` are both not missing.

**Instructions**

- Use `cor()` to compute the correlation between the birthweight of babies in the `ncbirths` dataset and their mother's age. There is no missing data in either variable.
- Compute the correlation between the birthweight and the number of weeks of gestation for all non-missing pairs.

```{r}
# Compute correlation
ncbirths %>%
  summarize(N = n(), r = cor(weight, mage))

# Compute correlation for all non-missing pairs
ncbirths %>%
  summarize(N = n(), r = cor(weight, weeks, use = 'pairwise.complete.obs'))

```

## **The Anscombe dataset**

**1. The Anscombe dataset**

In 1973, statistician Francis Anscombe created a synthetic dataset that has been used ever since to illustrate concepts related to correlation and regression. By discussing these with you, I will have fulfilled my obligation as a competent statistics instructor.

**2. Anscombe**

Here we see four datasets, each having a very different graphical presentation. However, these datasets have the same number of points, the same mean and standard deviation in both x and y, the same correlation, and the same regression line. How is it possible that four datasets with such obvious visual differences could have so many identical important properties?

```{r}
Anscombe %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() +
  facet_wrap(~ set)
anscombe
```

**3. Anscombe 1**

The first dataset looks the most like "real data." Here we see a positive, linear, moderately strong relationship. As you gain experience working with real data, you will see lots of scatter plots that look similar to this one. Here, the correlation coefficient of 0 (point) 82 is giving us an accurate measurement of the strength of the linear relationship between x and y.

```{r}
Anscombe %>% 
  filter(set == 1) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point()
```

**4. Anscombe 2**

However, in the next set we see something quite different. The relationship here is clearly nonlinear. But note also that while the value of the correlation coefficient is 0 (point) 82, the correlation, in an intuitive sense, is perfect. That is, knowing the value of x tells you the value of y exactly there is no statistical noise. It's just that the relationship is not linear.

```{r}
Anscombe %>% 
  filter(set == 2) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point()

```

**5. Anscombe 3**

In the third dataset, we have an outlier. Here again, our eyes tell us that with the exception of the outlier, the correlation between x and y is perfect, and in this case, it is linear. However, the presence of the outlier has lowered the value of the correlation coefficient and, as you will learn later, the slope of the regression line.

```{r}
Anscombe %>% 
  filter(set == 3) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point()

```

**6. Anscombe 4**

Finally, in the fourth dataset we see a pathological example with almost the opposite problem. In the previous plot, we saw that the presence of a single outlier had lowered the correlation from 1 to 0 (point) 82. In this plot, the presence of a single outlier raises the correlation from undefined to 0 (point) 82. Note that here, with the exception of the outlier, knowing x doesn't tell you anything about y. It is only the outlier that gives you the appearance of a correlation, but that correlation is specious. The Anscombe datasets are fabricated, but they help to illustrate some properties of correlation. Above all else, these examples help to underscore the importance of visually inspecting your data! You never know what you are going to see.

```{r}
Anscombe %>% 
  filter(set == 4) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point()

```

**7. Let's practice!**

Now it's your turn to explore the Anscombe dataset.

### **Exploring Anscombe**

In 1973, Francis Anscombe famously created four datasets with remarkably similar numerical properties, but obviously different graphic relationships. The `Anscombe` dataset contains the `x` and `y` coordinates for these four datasets, along with a grouping variable, `set`, that distinguishes the quartet.

It may be helpful to remind yourself of the graphic relationship by viewing the four scatterplots:

```{r}
ggplot(data = Anscombe, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~ set)
```

**Instructions**

For each of the four `set`s of data points in the `Anscombe` dataset, compute the following in the order specified. Names are provided in your call to `summarize()`.

- Number of observations, `N`
- Mean of `x`
- Standard deviation of `x`
- Mean of `y`
- Standard deviation of `y`
- Correlation coefficient between `x` and `y`

```{r}
# Compute properties of Anscombe
Anscombe %>%
  group_by(set) %>%
  summarize(
    N = n(), 
    mean_of_x = mean(x), 
    std_dev_of_x = sd(x), 
    mean_of_y = mean(y), 
    std_dev_of_y = sd(y), 
    correlation_between_x_and_y = cor(x, y)
  )
```

### **Perception of correlation (2)**

Estimating the value of the correlation coefficient between two quantities from their scatterplot can be tricky. Statisticians have shown that people's perception of the strength of these relationships can be influenced by design choices like the x and y scales.

Nevertheless, with some practice your perception of correlation will improve. Toggle through the four scatterplots in the plotting window, each of which you've seen in a previous exercise. Jot down your best estimate of the value of the correlation coefficient between each pair of variables. Then, compare these values to the actual values you compute in this exercise.

If you're having trouble recalling variable names, it may help to preview a dataset in the console with `str()` or `glimpse()`.

**Instructions**

1. Draw the plot then calculate the correlation between `OBP` and `SLG` for all players in the `mlbBat10` dataset.
2. Draw the plot then calculate the correlation between `OBP` and `SLG` for all players in the `mlbBat10` dataset with at least 200 at-bats.
3. Draw the plot then calculate the correlation between height and weight for each sex in the `bdims` dataset.
4. Draw the plot then calculate the correlation between body weight and brain weight for all species of `mammals`. Alongside this computation, compute the correlation between the same two quantities after taking their natural logarithms.

```{r}
# Run this and look at the plot
ggplot(data = mlbBat10, aes(x = obp, y = slg)) +
  geom_point()

# Correlation for all baseball players
mlbBat10 %>%
  summarize(N = n(), r = cor(obp, slg))

# Run this and look at the plot
mlbBat10 %>% 
    filter(at_bat > 200) %>%
    ggplot(aes(x = obp, y = slg)) + 
    geom_point()

# Correlation for all players with at least 200 ABs
mlbBat10 %>%
  filter(at_bat >= 200) %>%
  summarize(N = n(), r = cor(obp, slg))

# Run this and look at the plot
ggplot(data = bdims, aes(x = hgt, y = wgt, color = factor(sex))) +
  geom_point() 

# Correlation of body dimensions
bdims %>%
  group_by(sex) %>%
  summarize(N = n(), r = cor(hgt, wgt))

# Run this and look at the plot
ggplot(data = mammals, aes(x = body_wt, y = brain_wt)) +
  geom_point() + scale_x_log10() + scale_y_log10()

# Correlation among mammals, with and without log
mammals %>%
  summarize(N = n(), 
            r = cor(brain_wt, body_wt), 
            r_log = cor(log(body_wt), log(brain_wt)))
```

## **Interpretation of correlation**

**1. Interpretation of Correlation**

The word "correlation" has both a precise mathematical definition and a more general definition for typical usage in English. While these uses of the word are obviously related and generally in sync, there are times when these two uses can be conflated and or misconstrued. This occurs frequently in the media when journalists write about scientific results, particularly in health-related studies.

**2. Exercise and beer**

For example, consider the following article from the New York Times about the connection between exercise and drinking beer. After discussing some relatable scenarios, the lead-in raises the question of causation:

1 Source: http://well.blogs.nytimes.com/2015/12/02/the-close-ties-between-exercise-and-beer/

"But whether exercise encourages people to drink and, likewise, whether drinking encourages people to exercise has been in dispute." But causation is not really the goal here.

1 Source: http://well.blogs.nytimes.com/2015/12/02/the-close-ties-between-exercise-and-beer/

Notice how in the next two paragraphs, verbs linking exercise and beer-drinking include:

1 Source: http://well.blogs.nytimes.com/2015/12/02/the-close-ties-between-exercise-and-beer/

"influence",

1 Source: http://well.blogs.nytimes.com/2015/12/02/the-close-ties-between-exercise-and-beer/

"affect", and "tend to". The

1 Source: http://well.blogs.nytimes.com/2015/12/02/the-close-ties-between-exercise-and-beer/

"interplay between" the two activities is also mentioned.

1 Source: http://well.blogs.nytimes.com/2015/12/02/the-close-ties-between-exercise-and-beer/

In the end, this article is worded fairly carefully. Although many different possible connections between exercising and drinking are explored, the author has taken steps not to imply that the research demonstrates that exercising causes one to drink nor that drinking causes one to exercise.

The best interpretation is quoted from the actual study: "people drank more than usual on the same days that they engaged in more physical activity than usual."

Note how this statement makes it clear that an association was observed, but does not try to imply causality. In observational studies, which are especially common in nutritional science, health, and epidemiology, one must always be careful not to erroneously suggest that correlation implies causation.

**3. NFL arrests**

Here is another interesting use of correlation from the New York Times. The article examines the arrest records of National Football League players and, among other things, considers whether certain franchises tend to have more or fewer players arrested consistently over time. This use of correlation often called serial correlation or autocorrelation checks to see whether a single numeric variable is highly correlated with past measurements of itself. From the plot and the reported 0 (point) 53 correlation, we see some evidence that the number of players arrested from each team during the seven-year period from 2000 to 2006 was positively associated with the number of players arrested by that same team during the subsequent seven-year period from 2007 to 2013.

1 Source: https://www.nytimes.com/2014/09/13/upshot/what-the-numbers-show-about-nfl-player-arrests.html

The author interprets both the plot and reported a "pretty solid" correlation as evidence of a team-specific effect. Without offering explanations or implying cause-and-effect, the author effectively communicates that player arrests in the NFL do not appear to be randomly distributed across teams, but rather that certain teams may have player acquisition philosophies that result in them having consistently higher numbers of players arrested.

**4. Correlation vs. regression**

Sometimes journalists can leave statistical concepts lost in translation. The first sentence of this article

1 Source: http://www.nytimes.com/2012/11/02/business/questions-raised-on-withdrawal-of-congressional-research-services-report-on-tax-rates.html

reports that no correlation was found between economic growth and lower tax rates at the top of the income bracket. This is a politically sensitive finding and indeed much of the article discusses

1 Source: http://www.nytimes.com/2012/11/02/business/questions-raised-on-withdrawal-of-congressional-research-services-report-on-tax-rates.html

attempts by Senate Republicans to bury these findings, which run counter to their ideological position on macroeconomic issues. Of greater interest to us is that the word "correlation" here is not being used in its precise statistical sense. The link to the actual report reveals that the findings are based on a multiple regression model, not a simple correlation. Multiple regression models are the subject of the next course, but for now, it is enough to know that multiple regression is a technique that extends regression such that more than one explanatory variable can be taken into account. This is impossible to do with correlation, which is a simple, bivariate statistic.

1 Source: http://www.nytimes.com/2012/11/02/business/questions-raised-on-withdrawal-of-congressional-research-services-report-on-tax-rates.html

**5. Can you plot a correlation?**

Finally, it is not uncommon to encounter references to a "plot" of a correlation. This doesn't quite make sense. Of course, a scatter plot may be a graphical presentation of two variables and the strength of the linear relationship shown can be neatly summarized by a correlation. In this case, the article refers to a table of correlations between several variables often called a correlation matrix as a "plot" of a correlation. The conclusion that there is no correlation between voting for Brexit and racism is supported by these small correlation values. What other evidence would you want to see in order to have confidence in these findings?

1 Source: http://heatst.com/world/no-correlation-between-voting-for-brexit-and-racism-study-finds/

**6. Let's practice!**

The next few exercises will test your ability to interpret correlation like a pro.

## **Spurius correlations**

**1. Spurious correlations**

Above all else, we must always remember that correlation does not imply causation. That is, just because two variables appear to move together does not mean that changes in one are causing changes in the other. There are many potential confounders that can cloud our ability to determine cause-and-effect. Remarkable but nonsensical correlations are called "spurious."

**2. Spurious over time**

In fact, there is an entire blog devoted to them. Here is how the number of films Nicolas Cage has appeared in correlates with the number of people who drowned by falling into a pool. Are these two variables related by any substantive means? Of course not.

**3. Spurious over time**

Here is another example that traces U.S. oil production to the quality of rock music. I suppose one may need to burn the midnight oil to make a truly endearing rock classic. In each of these cases, the correlation was illustrated as two variables that seem to move together over time. In fact, time is an obvious confounder for many of these spurious correlations. Any time you see two variables linked over time, you should be skeptical of the role that time can play as a confounder.

**4. Spurious over space**

This webcomic from XKCD illustrates how space can also be present in spurious correlations. Colored maps, called choropleths, can be visually arresting ways of conveying information, but they can also reveal spurious correlations. Many things occur more often in places where there are more people, so failure to control for the confounding influence of population can lead to spurious correlations.

**5. Spurious for whatever reason**

In other cases it may not be so obvious what confounding variables are driving the spurious correlation. This plot shows that as the number of lemons imported into the U.S. from Mexico has increased over time, the number of U.S. highway fatalities has decreased. The relationship appears to be strong, but of course, there is no plausible causative mechanism that could explain the relationship. This is most likely simply due to chance. The scrupulous statistician must always be on the lookout for such spurious correlations.

**6. Let's practice!**

Now, it's time for you to get your hands dirty with some truly spurious correlations.

### **Spurious correlation in random data**

Statisticians must always be skeptical of potentially spurious correlations. Human beings are very good at seeing patterns in data, sometimes when the patterns themselves are actually just random noise. To illustrate how easy it can be to fall into this trap, we will look for patterns in truly random data.

The `noise` dataset contains 20 sets of `x` and `y` variables drawn at random from a standard normal distribution. Each set, denoted as `z`, has 50 observations of `x`, `y` pairs. Do you see any pairs of variables that might be meaningfully correlated? Are all of the correlation coefficients close to zero?

**Instructions**

- Create a faceted scatterplot that shows the relationship between each of the 20 sets of pairs of random variables `x` and `y`. You will need the `facet_wrap()` function for this.
- Compute the actual correlation between each of the 20 sets of pairs of `x` and `y`.
- Identify the datasets that show non-trivial correlation of greater than 0.2 in absolute value.

```{r}
# Create faceted scatterplot
noise %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~ z)

# Compute correlations for each dataset
noise_summary <- noise %>%
  group_by(z) %>%
  summarize(N = n(), spurious_cor = cor(x, y))

# Isolate sets with correlations above 0.2 in absolute strength
noise_summary %>%
  filter(abs(spurious_cor) > 0.2)
```

### **
